{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D, LeakyReLU\n",
    "from tensorflow.keras.layers import  Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "from random import shuffle\n",
    "import voxel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "def dataload():\n",
    "    img = np.load(\"imgs.npy\")\n",
    "    mask = np.load(\"masks.npy\")\n",
    "    \n",
    "    # shuffle\n",
    "    shuffle = np.arange(img.shape[0])\n",
    "    np.random.shuffle(shuffle)\n",
    "    img = img[shuffle]\n",
    "    mask = mask[shuffle]\n",
    "\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1) \n",
    "#     mask = to_categorical(mask) \n",
    "    \n",
    "    return img, mask \n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    yt0 = y_true[:,:,:,0]\n",
    "    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
    "    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
    "    union = tf.math.count_nonzero(tf.add(yt0, yp0))\n",
    "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
    "    return iou\n",
    "\n",
    "# def minmaxScaler(data):\n",
    "\n",
    "# def dice_coef(y_true, y_pred, smooth=1):\n",
    "#     y_pred = K.argmax(y_pred, axis=-1)\n",
    "# #     y_true = y_true[:,:,:,0]\n",
    "\n",
    "#     y_true_f = K.flatten(y_true)\n",
    "#     y_pred_f = K.flatten(y_pred)\n",
    "#     y_true_f = K.cast(y_true_f, 'float32')\n",
    "#     y_pred_f = K.cast(y_pred_f, 'float32')\n",
    "\n",
    "#     intersection = K.sum(y_true_f * y_pred_f)\n",
    "#     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_multilabel(y_true, y_pred, numLabels=3):\n",
    "    dice = 0\n",
    "\n",
    "    for index in range(numLabels):\n",
    "        if index == 0: continue\n",
    "        dice += dice_coef_each(y_true, y_pred, index)\n",
    "        \n",
    "    return dice / numLabels # taking average\n",
    "\n",
    "def dice_coef_each(y_true, y_pred, label, smooth=1):\n",
    "    y_true = K.cast(K.equal(y_true, label), 'float32')\n",
    "    y_pred = K.cast(K.equal(K.argmax(y_pred, axis=-1), label), 'float32')\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_0(y_true, y_pred):\n",
    "    return dice_coef_each(y_true, y_pred, 0)\n",
    "\n",
    "def dice_coef_1(y_true, y_pred):\n",
    "    return dice_coef_each(y_true, y_pred, 1)\n",
    "\n",
    "def dice_coef_2(y_true, y_pred):\n",
    "    return dice_coef_each(y_true, y_pred, 2)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_pred = K.argmax(y_pred, axis=-1)\n",
    "    y_true = y_true[:,:,:,1]\n",
    "    \n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    y_true_f = K.cast(y_true_f, 'float32')\n",
    "    y_pred_f = K.cast(y_pred_f, 'float32')\n",
    "    \n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return tf.math.exp(1  - score) - 1.0\n",
    "    # return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def build_unet(sz=(512,512,1)):\n",
    "    x = Input(sz)\n",
    "    inputs = x\n",
    "  \n",
    "    #down sampling \n",
    "    f = 8\n",
    "    layers = []\n",
    "  \n",
    "    for i in range(0, 6):\n",
    "        # kernel_initializer='he_norm' kernel의 값을 맞춰줄 수 있음. (초기화 설정) he_norm 앞 레이어의 평균과 표준편차를 맞춰서 정규화를 해준다.\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "        # bias는 BatchNormalization에서 조절\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', use_bias=False, kernel_initializer='he_normal') (x)\n",
    "        # BatchNormalization\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        layers.append(x)\n",
    "        x = MaxPooling2D() (x)\n",
    "        f = f*2\n",
    "        ff2 = 64 \n",
    "    \n",
    "    #bottleneck \n",
    "    j = len(layers) - 1\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
    "    x = Concatenate(axis=3)([x, layers[j]])\n",
    "    j = j -1 \n",
    "  \n",
    "    #upsampling \n",
    "    for i in range(0, 5):\n",
    "        ff2 = ff2//2\n",
    "        f = f // 2 \n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "        x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "        x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same', use_bias=False) (x)\n",
    "        # BatchNormalization\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = Concatenate(axis=3)([x, layers[j]])\n",
    "        j = j -1 \n",
    "    \n",
    "    #classification \n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Conv2D(f, 3, activation='relu', padding='same', kernel_initializer='he_normal') (x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    outputs = Conv2D(3, 1, activation='softmax') (x)\n",
    "    \n",
    "    #model creation \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', \n",
    "                  metrics = [dice_coef_0, dice_coef_1, dice_coef_2])\n",
    "  \n",
    "    return model\n",
    "# model = build_unet()\n",
    "# model.save('./model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3890, 512, 512, 1) (3890, 512, 512, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 8)  80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 8)  576         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512, 512, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512, 512, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 256, 256, 8)  0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 16) 1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 16) 2304        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 16) 64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256, 256, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 16) 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 32) 4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 32) 9216        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 32) 128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 32)   0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 64)   36864       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 64)   0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 128)  73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 128)  147456      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 128)  0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 256)  295168      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 256)  589824      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16, 16, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 256)    0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 512)    1180160     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 8, 8, 512)    0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 64)   131136      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 320)  0           conv2d_transpose[0][0]           \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 256)  737536      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 256)  590080      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 32)   32768       conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 160)  0           dropout_7[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 128)  184448      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 128)  147584      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 16)   8192        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 16)   64          conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 64, 16)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 80)   0           dropout_8[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 64)   46144       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 64)   36928       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 8)  2048        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 8)  32          conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128, 128, 8)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 40) 0           dropout_9[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 32) 11552       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 32) 9248        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 4)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256, 256, 4)  16          conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256, 256, 4)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256, 256, 20) 0           dropout_10[0][0]                 \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 16) 2896        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 16) 2320        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 512, 512, 2)  128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 512, 512, 2)  8           conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512, 512, 2)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512, 512, 10) 0           dropout_11[0][0]                 \n",
      "                                                                 dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 512, 512, 16) 1456        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 512, 512, 16) 2320        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 512, 512, 16) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 512, 512, 3)  51          dropout_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 6,669,227\n",
      "Trainable params: 6,668,095\n",
      "Non-trainable params: 1,132\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0741 - dice_coef_0: 0.9874 - dice_coef_1: 0.4353 - dice_coef_2: 0.4748\n",
      "Epoch 00001: val_loss improved from inf to 0.04550, saving model to ./model/1_0.0455.h5\n",
      "389/389 [==============================] - 62s 160ms/step - loss: 0.0741 - dice_coef_0: 0.9874 - dice_coef_1: 0.4353 - dice_coef_2: 0.4748 - val_loss: 0.0455 - val_dice_coef_0: 0.9897 - val_dice_coef_1: 0.6468 - val_dice_coef_2: 0.6086\n",
      "Epoch 2/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0254 - dice_coef_0: 0.9968 - dice_coef_1: 0.7777 - dice_coef_2: 0.7554\n",
      "Epoch 00002: val_loss improved from 0.04550 to 0.02088, saving model to ./model/2_0.0209.h5\n",
      "389/389 [==============================] - 62s 160ms/step - loss: 0.0254 - dice_coef_0: 0.9968 - dice_coef_1: 0.7777 - dice_coef_2: 0.7554 - val_loss: 0.0209 - val_dice_coef_0: 0.9965 - val_dice_coef_1: 0.7889 - val_dice_coef_2: 0.7739\n",
      "Epoch 3/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0169 - dice_coef_0: 0.9983 - dice_coef_1: 0.8210 - dice_coef_2: 0.8065\n",
      "Epoch 00003: val_loss improved from 0.02088 to 0.01537, saving model to ./model/3_0.0154.h5\n",
      "389/389 [==============================] - 62s 159ms/step - loss: 0.0169 - dice_coef_0: 0.9983 - dice_coef_1: 0.8210 - dice_coef_2: 0.8065 - val_loss: 0.0154 - val_dice_coef_0: 0.9979 - val_dice_coef_1: 0.8439 - val_dice_coef_2: 0.8279\n",
      "Epoch 4/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0138 - dice_coef_0: 0.9986 - dice_coef_1: 0.8533 - dice_coef_2: 0.8199\n",
      "Epoch 00004: val_loss improved from 0.01537 to 0.01382, saving model to ./model/4_0.0138.h5\n",
      "389/389 [==============================] - 60s 154ms/step - loss: 0.0138 - dice_coef_0: 0.9986 - dice_coef_1: 0.8533 - dice_coef_2: 0.8199 - val_loss: 0.0138 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.8535 - val_dice_coef_2: 0.8954\n",
      "Epoch 5/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0131 - dice_coef_0: 0.9987 - dice_coef_1: 0.8510 - dice_coef_2: 0.8242\n",
      "Epoch 00005: val_loss improved from 0.01382 to 0.00979, saving model to ./model/5_0.0098.h5\n",
      "389/389 [==============================] - 61s 156ms/step - loss: 0.0131 - dice_coef_0: 0.9987 - dice_coef_1: 0.8510 - dice_coef_2: 0.8242 - val_loss: 0.0098 - val_dice_coef_0: 0.9989 - val_dice_coef_1: 0.8977 - val_dice_coef_2: 0.9088\n",
      "Epoch 6/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0112 - dice_coef_0: 0.9989 - dice_coef_1: 0.8627 - dice_coef_2: 0.8616\n",
      "Epoch 00006: val_loss did not improve from 0.00979\n",
      "389/389 [==============================] - 61s 156ms/step - loss: 0.0112 - dice_coef_0: 0.9989 - dice_coef_1: 0.8627 - dice_coef_2: 0.8616 - val_loss: 0.0112 - val_dice_coef_0: 0.9988 - val_dice_coef_1: 0.8957 - val_dice_coef_2: 0.9256\n",
      "Epoch 7/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0106 - dice_coef_0: 0.9989 - dice_coef_1: 0.8740 - dice_coef_2: 0.8696\n",
      "Epoch 00007: val_loss did not improve from 0.00979\n",
      "389/389 [==============================] - 61s 157ms/step - loss: 0.0106 - dice_coef_0: 0.9989 - dice_coef_1: 0.8740 - dice_coef_2: 0.8696 - val_loss: 0.0129 - val_dice_coef_0: 0.9987 - val_dice_coef_1: 0.8902 - val_dice_coef_2: 0.9179\n",
      "Epoch 8/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0097 - dice_coef_0: 0.9990 - dice_coef_1: 0.8792 - dice_coef_2: 0.8580\n",
      "Epoch 00008: val_loss did not improve from 0.00979\n",
      "389/389 [==============================] - 61s 156ms/step - loss: 0.0097 - dice_coef_0: 0.9990 - dice_coef_1: 0.8792 - dice_coef_2: 0.8580 - val_loss: 0.0321 - val_dice_coef_0: 0.9980 - val_dice_coef_1: 0.8374 - val_dice_coef_2: 0.9215\n",
      "Epoch 9/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0087 - dice_coef_0: 0.9991 - dice_coef_1: 0.8918 - dice_coef_2: 0.8808\n",
      "Epoch 00009: val_loss did not improve from 0.00979\n",
      "389/389 [==============================] - 61s 157ms/step - loss: 0.0087 - dice_coef_0: 0.9991 - dice_coef_1: 0.8918 - dice_coef_2: 0.8808 - val_loss: 0.0139 - val_dice_coef_0: 0.9987 - val_dice_coef_1: 0.8812 - val_dice_coef_2: 0.9233\n",
      "Epoch 10/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0095 - dice_coef_0: 0.9990 - dice_coef_1: 0.8682 - dice_coef_2: 0.8667\n",
      "Epoch 00010: val_loss did not improve from 0.00979\n",
      "389/389 [==============================] - 61s 158ms/step - loss: 0.0095 - dice_coef_0: 0.9990 - dice_coef_1: 0.8682 - dice_coef_2: 0.8667 - val_loss: 0.0228 - val_dice_coef_0: 0.9982 - val_dice_coef_1: 0.7998 - val_dice_coef_2: 0.8890\n",
      "Epoch 11/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0090 - dice_coef_0: 0.9991 - dice_coef_1: 0.8786 - dice_coef_2: 0.8681\n",
      "Epoch 00011: val_loss did not improve from 0.00979\n",
      "389/389 [==============================] - 61s 157ms/step - loss: 0.0090 - dice_coef_0: 0.9991 - dice_coef_1: 0.8786 - dice_coef_2: 0.8681 - val_loss: 0.0457 - val_dice_coef_0: 0.9976 - val_dice_coef_1: 0.8192 - val_dice_coef_2: 0.8796\n",
      "Epoch 12/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0075 - dice_coef_0: 0.9992 - dice_coef_1: 0.8801 - dice_coef_2: 0.8509\n",
      "Epoch 00012: val_loss did not improve from 0.00979\n",
      "389/389 [==============================] - 60s 155ms/step - loss: 0.0075 - dice_coef_0: 0.9992 - dice_coef_1: 0.8801 - dice_coef_2: 0.8509 - val_loss: 0.0257 - val_dice_coef_0: 0.9983 - val_dice_coef_1: 0.8732 - val_dice_coef_2: 0.9059\n",
      "Epoch 13/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0079 - dice_coef_0: 0.9992 - dice_coef_1: 0.8873 - dice_coef_2: 0.8672\n",
      "Epoch 00013: val_loss improved from 0.00979 to 0.00792, saving model to ./model/13_0.0079.h5\n",
      "389/389 [==============================] - 61s 156ms/step - loss: 0.0079 - dice_coef_0: 0.9992 - dice_coef_1: 0.8873 - dice_coef_2: 0.8672 - val_loss: 0.0079 - val_dice_coef_0: 0.9991 - val_dice_coef_1: 0.9102 - val_dice_coef_2: 0.9219\n",
      "Epoch 14/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0097 - dice_coef_0: 0.9990 - dice_coef_1: 0.8753 - dice_coef_2: 0.8682\n",
      "Epoch 00014: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 61s 156ms/step - loss: 0.0097 - dice_coef_0: 0.9990 - dice_coef_1: 0.8753 - dice_coef_2: 0.8682 - val_loss: 0.0153 - val_dice_coef_0: 0.9986 - val_dice_coef_1: 0.8842 - val_dice_coef_2: 0.9178\n",
      "Epoch 15/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0076 - dice_coef_0: 0.9992 - dice_coef_1: 0.8813 - dice_coef_2: 0.8742\n",
      "Epoch 00015: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 61s 157ms/step - loss: 0.0076 - dice_coef_0: 0.9992 - dice_coef_1: 0.8813 - dice_coef_2: 0.8742 - val_loss: 0.0195 - val_dice_coef_0: 0.9988 - val_dice_coef_1: 0.8733 - val_dice_coef_2: 0.8679\n",
      "Epoch 16/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0070 - dice_coef_0: 0.9993 - dice_coef_1: 0.8919 - dice_coef_2: 0.8745\n",
      "Epoch 00016: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 61s 156ms/step - loss: 0.0070 - dice_coef_0: 0.9993 - dice_coef_1: 0.8919 - dice_coef_2: 0.8745 - val_loss: 0.0270 - val_dice_coef_0: 0.9985 - val_dice_coef_1: 0.8597 - val_dice_coef_2: 0.9183\n",
      "Epoch 17/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0069 - dice_coef_0: 0.9993 - dice_coef_1: 0.9046 - dice_coef_2: 0.8783\n",
      "Epoch 00017: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 60s 154ms/step - loss: 0.0069 - dice_coef_0: 0.9993 - dice_coef_1: 0.9046 - dice_coef_2: 0.8783 - val_loss: 0.0158 - val_dice_coef_0: 0.9988 - val_dice_coef_1: 0.8919 - val_dice_coef_2: 0.9331\n",
      "Epoch 18/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0068 - dice_coef_0: 0.9993 - dice_coef_1: 0.9037 - dice_coef_2: 0.8770\n",
      "Epoch 00018: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 60s 155ms/step - loss: 0.0068 - dice_coef_0: 0.9993 - dice_coef_1: 0.9037 - dice_coef_2: 0.8770 - val_loss: 0.0097 - val_dice_coef_0: 0.9992 - val_dice_coef_1: 0.9101 - val_dice_coef_2: 0.9319\n",
      "Epoch 19/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0070 - dice_coef_0: 0.9992 - dice_coef_1: 0.8895 - dice_coef_2: 0.8861\n",
      "Epoch 00019: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 61s 156ms/step - loss: 0.0070 - dice_coef_0: 0.9992 - dice_coef_1: 0.8895 - dice_coef_2: 0.8861 - val_loss: 0.0115 - val_dice_coef_0: 0.9990 - val_dice_coef_1: 0.9064 - val_dice_coef_2: 0.9210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0065 - dice_coef_0: 0.9993 - dice_coef_1: 0.8891 - dice_coef_2: 0.8743\n",
      "Epoch 00020: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 60s 155ms/step - loss: 0.0065 - dice_coef_0: 0.9993 - dice_coef_1: 0.8891 - dice_coef_2: 0.8743 - val_loss: 0.0091 - val_dice_coef_0: 0.9991 - val_dice_coef_1: 0.9140 - val_dice_coef_2: 0.9294\n",
      "Epoch 21/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0062 - dice_coef_0: 0.9993 - dice_coef_1: 0.9027 - dice_coef_2: 0.8898\n",
      "Epoch 00021: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 60s 155ms/step - loss: 0.0062 - dice_coef_0: 0.9993 - dice_coef_1: 0.9027 - dice_coef_2: 0.8898 - val_loss: 0.0127 - val_dice_coef_0: 0.9991 - val_dice_coef_1: 0.9086 - val_dice_coef_2: 0.9217\n",
      "Epoch 22/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0074 - dice_coef_0: 0.9992 - dice_coef_1: 0.8829 - dice_coef_2: 0.8714\n",
      "Epoch 00022: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 60s 154ms/step - loss: 0.0074 - dice_coef_0: 0.9992 - dice_coef_1: 0.8829 - dice_coef_2: 0.8714 - val_loss: 0.0217 - val_dice_coef_0: 0.9986 - val_dice_coef_1: 0.8797 - val_dice_coef_2: 0.9057\n",
      "Epoch 23/100\n",
      "389/389 [==============================] - ETA: 0s - loss: 0.0063 - dice_coef_0: 0.9993 - dice_coef_1: 0.9058 - dice_coef_2: 0.8964\n",
      "Epoch 00023: val_loss did not improve from 0.00792\n",
      "389/389 [==============================] - 61s 158ms/step - loss: 0.0063 - dice_coef_0: 0.9993 - dice_coef_1: 0.9058 - dice_coef_2: 0.8964 - val_loss: 0.0195 - val_dice_coef_0: 0.9987 - val_dice_coef_1: 0.8882 - val_dice_coef_2: 0.9066\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "    # source, target = data_convert()\n",
    "\n",
    "    source, target = dataload()\n",
    "    print(source.shape, target.shape)\n",
    "    \n",
    "    model = build_unet()\n",
    "    model.summary()\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    model_checkpoint = ModelCheckpoint(filepath='./model/{epoch}_{val_loss:.4f}.h5',\n",
    "                                      monitor='val_loss',\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1,\n",
    "                                      mode='auto')\n",
    "    model.fit(source, target, epochs=100, validation_split=0.2, batch_size=8, callbacks=[early_stopping, model_checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
